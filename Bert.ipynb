{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFCzIBshcHmg"
      },
      "source": [
        "# **Final Project**\n",
        "\n",
        "##**Problem stament :**     \n",
        "\n",
        "The widespread dissemination of fake news and propaganda presents serious societal risks, including the erosion of public trust, political polarization, manipulation of elections, and the spread of harmful misinformation during crises such as pandemics or conflicts. From an NLP perspective, detecting fake news is fraught with challenges. Linguistically, fake news often mimics the tone and structure of legitimate journalism, making it difficult to distinguish using surface-level features. The absence of reliable and up-to-date labeled datasets, especially across multiple languages and regions, hampers the effectiveness of supervised learning models. Additionally, the dynamic and adversarial nature of misinformation means that malicious actors constantly evolve their language and strategies to bypass detection systems. Cultural context, sarcasm, satire, and implicit bias further complicate automated analysis. Moreover, NLP models risk amplifying biases present in training data, leading to unfair classifications and potential censorship of legitimate content. These challenges underscore the need for cautious, context-aware approaches, as the failure to address them can inadvertently contribute to misinformation, rather than mitigate it.\n",
        "\n",
        "\n",
        "\n",
        "Use datasets in link : https://drive.google.com/drive/folders/1mrX3vPKhEzxG96OCPpCeh9F8m_QKCM4z?usp=sharing\n",
        "to complete requirement.\n",
        "\n",
        "## **About dataset:**\n",
        "\n",
        "* **True Articles**:\n",
        "\n",
        "  * **File**: `MisinfoSuperset_TRUE.csv`\n",
        "  * **Sources**:\n",
        "\n",
        "    * Reputable media outlets like **Reuters**, **The New York Times**, **The Washington Post**, etc.\n",
        "\n",
        "* **Fake/Misinformation/Propaganda Articles**:\n",
        "\n",
        "  * **File**: `MisinfoSuperset_FAKE.csv`\n",
        "  * **Sources**:\n",
        "\n",
        "    * **American right-wing extremist websites** (e.g., Redflag Newsdesk, Breitbart, Truth Broadcast Network)\n",
        "    * **Public dataset** from:\n",
        "\n",
        "      * Ahmed, H., Traore, I., & Saad, S. (2017): \"Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques\" *(Springer LNCS 10618)*\n",
        "\n",
        "\n",
        "\n",
        "## **Requirement**\n",
        "\n",
        "A team consisting of three members must complete a project that involves applying the methods learned from the beginning of the course up to the present. The team is expected to follow and document the entire machine learning workflow, which includes the following steps:\n",
        "\n",
        "1. **Data Preprocessing**: Clean and prepare the dataset,etc.\n",
        "\n",
        "2. **Exploratory Data Analysis (EDA)**: Explore and visualize the data.\n",
        "\n",
        "3. **Model Building**: Select and build one or more machine learning models suitable for the problem at hand.\n",
        "\n",
        "4. **Hyperparameter set up**: Set and adjust the model's hyperparameters using appropriate methods to improve performance.\n",
        "\n",
        "5. **Model Training**: Train the model(s) on the training dataset.\n",
        "\n",
        "6. **Performance Evaluation**: Evaluate the trained model(s) using appropriate metrics (e.g., accuracy, precision, recall, F1-score, confusion matrix, etc.) and validate their performance on unseen data.\n",
        "\n",
        "7. **Conclusion**: Summarize the results, discuss the model's strengths and weaknesses, and suggest possible improvements or future work.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  label  text_length\n",
            "0  link imo happening guys jumping conclusions co...      1         2717\n",
            "1  house passed budget today slim margin democrat...      1         3203\n",
            "2  youtube video posted january cloudgate studio ...      0         2549\n",
            "3  marseille france sang danced chanted even reas...      0         8149\n",
            "4  sunday nbc meet press discussing president don...      0         1235\n",
            "(68604, 3)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive - VNU-HCMUS\\cleaned\")\n",
        "print(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "0",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "bcb6aae7-1231-41d7-a8e3-25e9de24dc05",
              "rows": [
                [
                  "text",
                  "7"
                ],
                [
                  "label",
                  "0"
                ],
                [
                  "text_length",
                  "0"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 3
              }
            },
            "text/plain": [
              "text           7\n",
              "label          0\n",
              "text_length    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ki·ªÉm tra d·ªØ li·ªáu r·ªóng\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lo·∫°i b·ªè c√°c h√†ng c√≥ gi√° tr·ªã NaN trong c·ªôt 'text'\n",
        "df = df.dropna(subset=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chia d·ªØ li·ªáu th√†nh 3 ph·∫ßn: train, valid, test\n",
        "# 60% train, 20% valid, 20% test\n",
        "train_df = df[:int(0.2 * len(df))]\n",
        "valid_df = df[int(0.2 * len(df)):int(0.4 * len(df))]\n",
        "test_df = df[int(0.4 * len(df)):]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34bec92ddd4f4c5288813db66dd0692f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/13719 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9c0d2975f644a2e8ec8e020be9ed946",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/13719 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f3a6966941b4ceab795e420dc1be75b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/41159 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tokenize d∆∞ÃÉ li√™Ã£u\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import XLNetTokenizerFast\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, model_name, max_length=256):\n",
        "        if 'bert' in model_name:\n",
        "            self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "        elif 'xlnet' in model_name:\n",
        "            self.tokenizer = XLNetTokenizerFast.from_pretrained(model_name)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "tokenizer = Tokenizer('bert-base-uncased')\n",
        "\n",
        "# Convert DataFrames to HuggingFace Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "valid_dataset = Dataset.from_pandas(valid_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(tokenizer.tokenize_function, batched=True)\n",
        "valid_dataset = valid_dataset.map(tokenizer.tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenizer.tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "valid_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build and Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, XLNetForSequenceClassification\n",
        "\n",
        "class ModelBuilder:\n",
        "    def __init__(self, num_labels=2):\n",
        "        self.num_labels = num_labels\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self, model_name):\n",
        "        if 'bert' in model_name:\n",
        "            self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=self.num_labels)\n",
        "        elif 'xlnet' in model_name:\n",
        "            self.model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=self.num_labels)\n",
        "        return self.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_builder = ModelBuilder()\n",
        "bert_model = model_builder.build_model('bert-base-uncased')\n",
        "xlnet_model = model_builder.build_model('xlnet-base-cased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "xlnet_model.to(device)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'f1': float(f1),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall)\n",
        "    }\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer.tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./bert_results',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=2,\n",
        "    load_best_model_at_end=True,\n",
        "    dataloader_num_workers=8,\n",
        "    fp16=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c14c4f9b32f4636b2967d982cfc3232",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5145 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4966, 'grad_norm': 2.865341901779175, 'learning_rate': 4.517006802721089e-05, 'epoch': 0.1}\n",
            "{'loss': 0.3775, 'grad_norm': 51.16004943847656, 'learning_rate': 4.0310981535471334e-05, 'epoch': 0.19}\n",
            "{'loss': 0.3229, 'grad_norm': 6.9868292808532715, 'learning_rate': 3.546161321671526e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2944, 'grad_norm': 121.6175537109375, 'learning_rate': 3.0602526724975704e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2704, 'grad_norm': 0.05791429802775383, 'learning_rate': 2.5743440233236154e-05, 'epoch': 0.49}\n",
            "{'loss': 0.2576, 'grad_norm': 6.471226215362549, 'learning_rate': 2.0884353741496597e-05, 'epoch': 0.58}\n",
            "{'loss': 0.2283, 'grad_norm': 1.7582225799560547, 'learning_rate': 1.6044703595724002e-05, 'epoch': 0.68}\n",
            "{'loss': 0.2402, 'grad_norm': 0.12386985123157501, 'learning_rate': 1.1185617103984451e-05, 'epoch': 0.78}\n",
            "{'loss': 0.2174, 'grad_norm': 0.8838419318199158, 'learning_rate': 6.3265306122448975e-06, 'epoch': 0.87}\n",
            "{'loss': 0.2136, 'grad_norm': 0.38857510685920715, 'learning_rate': 1.467444120505345e-06, 'epoch': 0.97}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33802c7b4e6e4ce8b789d10b5632290b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1715 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.1977662593126297, 'eval_accuracy': 0.9532764778773963, 'eval_f1': 0.9533051801042925, 'eval_precision': 0.9538110833988157, 'eval_recall': 0.9532764778773963, 'eval_runtime': 125.9903, 'eval_samples_per_second': 108.889, 'eval_steps_per_second': 13.612, 'epoch': 1.0}\n",
            "{'train_runtime': 1357.8556, 'train_samples_per_second': 30.311, 'train_steps_per_second': 3.789, 'train_loss': 0.29085316560706315, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5145, training_loss=0.29085316560706315, metrics={'train_runtime': 1357.8556, 'train_samples_per_second': 30.311, 'train_steps_per_second': 3.789, 'total_flos': 5414562408253440.0, 'train_loss': 0.29085316560706315, 'epoch': 1.0})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training BERT model\n",
        "bert_trainer = Trainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    tokenizer=tokenizer.tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "bert_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2e89123d7f14e24a9c2ecb1b8023b86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1715 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 0.20656894147396088, 'eval_accuracy': 0.9505830903790088, 'eval_f1': 0.950670697794864, 'eval_precision': 0.9514131678200916, 'eval_recall': 0.9505830903790088, 'eval_runtime': 120.8432, 'eval_samples_per_second': 113.536, 'eval_steps_per_second': 14.192, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "eval_results = bert_trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(f\"Evaluation results: {eval_results}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
